{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhonrv1997/UPC/blob/main/PC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_8-ZriraBq5"
      },
      "source": [
        "# Práctica calificada 1\n",
        "\n",
        "Profesor: [Carlos Adrián Alarcón](https://www.linkedin.com/in/carlos-adrian-alarcon-delgado/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0iD2NfyvaBq7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7lgse6AaBq7"
      },
      "source": [
        "## Parte teórica (7 ptos.)\n",
        "\n",
        "Las preguntas tienen diferente nivel de dificultad. Respondan de manera precisa. Todos los dataset ya están cargados, así que ya no tienen que preocuparse por eso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lje7B_6AaBq7"
      },
      "source": [
        "### Implementación de modelo de regresión logística haciendo uso de gradiente descendente\n",
        "\n",
        "En el curso hemos conversado acerca del modelo de regresión logística y hemos explicado su funcionamiento en términos matemáticos. Ahora profundizaremos en ese aspecto. Para este ejercicio, intentaremos resolver el problema de clasificación mediante un modelo de regresión logística, estimando los parámetros a través la implementación del algoritmo de descenso de gradiente (este tema lo vamos a ver más adelante pero tómenlo como una buena introducción). Antes de iniciar, cabe recordar que partimos de que la variable de interés $Y$ se distribuye Bernoulli con parámetro p, donde p es la probabilidad de que $Y$ sea igual a 1, que en regresión logística se modela como:\n",
        "\n",
        "$$ p = P(Y=1) = \\frac{1}{1+exp\\left(-(\\beta_{0}+\\beta_{1}X)\\right)} = \\frac{exp\\left(\\beta_{0} + \\beta_{1}X\\right)}{1+exp\\left(\\beta_{0} + \\beta_{1}X\\right)} $$\n",
        "\n",
        "La estimación de los parámetros $\\beta^{T}=\\left[b, w_{1}, ...,w_{p}\\right]$ que definen el hiperplano se hace maximizando el logaritmo de la función de máxima verosimilitud (maximum likelihood) de los datos:\n",
        "\n",
        "\\begin{equation} \\arg \\max_{\\beta} \\log \\left(V \\left(\\beta \\right) \\right) = \\arg \\max_{\\beta} \\sum_{i=1}^{n} \\log \\left(p_{i}^{y_i}\\left(1-p_{i}\\right)^{1-y_i}\\right) \\end{equation}\n",
        "\n",
        "En este caso, para simplificar la notación hacemos $p_i = \\hat Y_i$. Dada esta función objetivo, nos hace falta definir ahora su gradiente, de modo que podamos proceder a implementar nuestro algortimo de descenso de gradiente para estimar los coeficientes. Aplicando propiedades de logaritmos:\n",
        "\n",
        "$$ \\sum_{i=1}^{n} \\log\\left(p_{i}^{y_i}\\left(1-p_{i}\\right)^{1-y_i}\\right) = \\sum_{i=1}^{n} \\left(y_i\\log\\left(p_{i}\\right) + \\left(1-y_i\\right)\\log\\left(1-p_{i}\\right)\\right) $$\n",
        "$$ = \\sum_{i=1}^{n} \\left(y_i\\log\\left(p_{i}\\right) + \\log\\left(1-p_{i}\\right) - y_i\\log\\left(1-p_{i}\\right)\\right) = \\sum_{i=1}^{n} \\left(y_i\\log\\left(\\frac{p_{i}}{1-p_{i}}\\right) + \\log\\left(1-p_{i}\\right)\\right)$$\n",
        "\n",
        "Es fácil demostrar que:\n",
        "\n",
        "$$ \\left(1-p\\right) = \\frac{1}{1+exp\\left(\\beta^{T}X\\right)} $$\n",
        "\n",
        "$$ \\log\\left(\\frac{p}{1-p}\\right) = \\beta^{T}X $$\n",
        "\n",
        "Por lo que, reemplazando, tenemos:\n",
        "\n",
        "$$ \\arg \\max_{\\beta} \\sum_{i=1}^{n} \\left(y_i\\beta^{T}x_i  + \\log\\left(\\frac{1}{1+exp\\left(\\beta^{T}x_i\\right)}\\right)\\right) $$\n",
        "\n",
        "Finalmente, nos queda derivar con respecto a cada parámetro, obteniendo:\n",
        "\n",
        "$$ \\frac {\\partial \\log \\left(L \\left(\\beta \\right) \\right)}{\\partial b} = \\sum_{i=1}^{n} \\left(y_i - p_{i}\\right) $$\n",
        "\n",
        "$$ \\frac {\\partial \\log \\left(L \\left(\\beta \\right) \\right)}{\\partial w_{j}} = \\sum_{i=1}^{n} \\left((y_i - p_{i})x_{ij}\\right) $$\n",
        "\n",
        "\n",
        "En las siguientes funciones se implementa el modelo de regresión logística. No tienen que modificar nada, solamente revisar el resultado final, que es la estimación de los coeficientes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PKyd8cUraBq8"
      },
      "outputs": [],
      "source": [
        "# Definir función objetivo - logaritmo de la función de verosimilitud\n",
        "def fobj(B, X, Y):\n",
        "    suma = 0\n",
        "    X=np.append(np.ones((len(X),1)), X, axis = 1) # Añadimos columna de 1s correspondiente al intercepto\n",
        "    for i in range(len(X)):\n",
        "        betaTx = np.matmul(np.transpose(B), X[i]) # Beta transpuesto por X\n",
        "        suma = suma + Y[i]*betaTx + np.log(1/(1+np.exp(betaTx)))\n",
        "    return suma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7bUlJv_jaBq8"
      },
      "outputs": [],
      "source": [
        "# Definir gradiente - vector con las derivadas parciales respecto a cada estimador\n",
        "def gradiente(B, X, Y):\n",
        "    gradiente = np.array([])\n",
        "    X=np.append(np.ones((len(X),1)), X, axis = 1) # Añadimos columna de 1s correspondiente al intercepto\n",
        "    for j in range(len(B)):\n",
        "        suma = 0\n",
        "        for i in range(len(X)):\n",
        "            betaTx = np.matmul(np.transpose(B), X[i]) # Beta transpuesto por X\n",
        "            pi = np.exp(betaTx)/(1+np.exp(betaTx))\n",
        "            suma = suma + (Y[i]-pi)*X[i][j]\n",
        "        gradiente=np.append(gradiente,suma) # Añadimos la derivada parcial con respecto a \\beta_j\n",
        "    return gradiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lj7_DmmFaBq8"
      },
      "outputs": [],
      "source": [
        "# Función de descenso de gradiente\n",
        "def regresion_logistica(fun_obj, gradiente, sol_inicial, step, tolerancia, X, Y):\n",
        "    # Cargar solución inicial\n",
        "    solucion = sol_inicial\n",
        "    # Definir variable booleana para validar si se cumplió el criterio de parada\n",
        "    stop = False\n",
        "    # Definir contador de iteraciones\n",
        "    num_iter = 1\n",
        "    while stop==False:\n",
        "        # Evaluar gradiente\n",
        "        gradiente_eval = gradiente(solucion, X, Y)\n",
        "        # Actualizar la solucion (se suma al tratarse de maximización)\n",
        "        solucion = solucion + step * gradiente_eval\n",
        "        # Evaluar la solucion\n",
        "        solucion_eval = fun_obj(solucion, X, Y)\n",
        "        # Actualizar contador de iteraciones\n",
        "        num_iter += 1\n",
        "        # Validar si se cumplió el criterio de parada, es decir, si la norma del gradiente evaluado en la solución actual es menor a la tolerancia dada\n",
        "        if (np.linalg.norm(gradiente_eval) < tolerancia):\n",
        "            stop = True\n",
        "    print(\"Luego de \"+str(num_iter)+\" iteraciones, se encontró que los estimadores del modelo de regresión logística son \"+str(solucion))\n",
        "    return solucion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQUk96l1aBq8",
        "outputId": "9070be78-2395-4731-f07e-220d05306f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Luego de 236 iteraciones, se encontró que los estimadores del modelo de regresión logística son [-0.66575338  0.76767828]\n",
            "[-0.66575338  0.76767828]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/aladelca/machine_learning_model/refs/heads/main/archivos_trabajo/data%20(1).csv')\n",
        "# Obtener Y\n",
        "YTotal=np.array(df['Purchased'])\n",
        "\n",
        "# Obtener X\n",
        "XTotal=np.array(df[['EstimatedSalary']])\n",
        "# Dividir muestra en train y test\n",
        "XTrain, XTest, yTrain, yTest = train_test_split(XTotal, YTotal, test_size=0.33, random_state=0)\n",
        "# Para facilitar convergencia, escalamos los datos\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(XTrain)\n",
        "XTrain=scaler.transform(XTrain)\n",
        "XTest=scaler.transform(XTest)\n",
        "\n",
        "\n",
        "# Definir solución inicial - estimadores en 0 - tenemos uno para cada variable independiente más el intercepto\n",
        "sol_inicial = np.zeros(len(XTrain[1])+1)\n",
        "# Definir tamaño de paso\n",
        "step = 0.001\n",
        "# Definir tolerancia\n",
        "tolerancia = 0.001\n",
        "\n",
        "# Llamar la función descenso de gradiente - tarda un poco\n",
        "estimadores = regresion_logistica(fobj, gradiente, sol_inicial, step, tolerancia, XTrain, yTrain)\n",
        "print(estimadores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bpWhVwaBq9"
      },
      "source": [
        "### Pregunta 1 (3 ptos.)\n",
        "\n",
        "En función a los coeficiente estimados por el algoritmo de regresión lógistica estimado con el método de gradiente descendente, calcular las probabilidades correspondientes (no utilizar SKLearn). Asegurate de calcular las probabilidades de compra y no compra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njTDcauPaBq9"
      },
      "outputs": [],
      "source": [
        "#### Escribe el código con la respuesta aquí ####\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdOKuKljaBq9"
      },
      "source": [
        "### Pregunta 2 (4 ptos.)\n",
        "\n",
        "Ahora con los mismos datos, entrena el modelo usando `SKLearn` usando solamente la variable `EstimatedSalary` (deberías obtener parámetros bastante similares). Genera las predicciones usando el índice de Youden para hallar el punto de corte (por ende, las predicciones). Calcula el `accuracy`, `sensitivity` y `specificity`, además de mostrar la matriz de confusión. También calcula el AUC y muestra la curva ROC. Comenta los principales hallazgos del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkVU8ki6aBq9"
      },
      "outputs": [],
      "source": [
        "### Escribe el código con la respuesta aquí ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_3Nv3o4aBq9"
      },
      "source": [
        "Escribe tus hallazgos del modelo aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lHXGcv2aBq9"
      },
      "source": [
        "Otro método para encontrar el threshold indicado es utilizar la curva `precision recall`. La idea es encontrar el threshold que haga que las métricas `precision` y `recall` sean iguales (o lo más parecidas posible). La métrica `precision` la puedes calcular con la función `precision_score` de `SKLearn`. Identifica el mejor threshold en función a este método y compáralo con el que vimos en clase (y que has calculado en la primera parte de esta pregunta)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoE8WsTqaBq9"
      },
      "outputs": [],
      "source": [
        "### Escribe el código con la respuesta aquí ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OoC1ppmaBq9"
      },
      "source": [
        "## Parte práctica (13 ptos.)\n",
        "\n",
        "Muchas veces estamos en trabajos que no nos gustan y pensamos en renunciar, ¿alguna vez se han puesto a preguntar cómo identificar aquellos empleados que están a punto de renunciar `attrition`? Para eso tenemos el siguiente [dataset](https://www.kaggle.com/datasets/vjchoudhary7/hr-analytics-case-study/data) y lo que tenemos que predecir es justamente `attrition`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn4AvKK5aBq9"
      },
      "source": [
        "### Pregunta 3 (2 ptos.)\n",
        "\n",
        "Un paso importante de cualquier modelo de data science es el famoso `feature engineering` o la creación de variables. Justamente, por eso, a partir de los datasets indicados crea un dataset con al menos 15 variables. Estas pueden ser simplemente unir los datasets haciendo uso de `merge` o la creación de otras variables a partir de las existentes (campos calculados, etc.). Explica claramente porque la variable es relevante para cualquier modelo de machine learning tomando en cuenta el caso de uso (predicción de `attrition`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1BFW-zWaBq9"
      },
      "outputs": [],
      "source": [
        "### Escribe el código aquí ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl4GNn6O-3bC"
      },
      "source": [
        "Escribe tus conclusiones y respuesta aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WVP2n2Wa4R2"
      },
      "source": [
        "### Pregunta 4 (3 ptos.)\n",
        "\n",
        "Genera un modelo lineal de machine learning para predecir la variable `attrition`. Analiza los coeficientes e indica qué variable es la más importante. Si en la preparación de tus datos tienes datos nulos, puedos usar la imputación (`SimpleImputer`), solamente tendrías que tener en cuenta la separación `train.` y /`test` para evitar leakage (ya hemos hablado bastante en clase acerca de porque hacemos la separación). Además, escribe la fórmula lineal y evalúa el modelo escogiendo una métrica adecuada. Justifica la elección de métrica en función al caso de uso (respuestas genéricas sin conexión al caso de uso tendrán automáticamente 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWFlzRmRc-IL"
      },
      "outputs": [],
      "source": [
        "### Escribe el código aquí ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZKtvyDldBNo"
      },
      "source": [
        "Escribe tus conclusiones y respuesta aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSlD4ZjbPuTZ"
      },
      "source": [
        "### Pregunta 5 (3 ptos.)\n",
        "\n",
        "Ahora entrena un modelo basado en `bagging`. Evalúalo con la métrica que escogiste en la pregunta anterior e indica qué modelo es mejor. Asimismo, explica qué es `bagging` y demuestra cómo es que está funcionando en tu modelo (mostrar unos ejemplos en código debería ser suficiente si es que sabes el concepto)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6HFx_kgPuTZ"
      },
      "outputs": [],
      "source": [
        "### Escribe el código aquí ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgAuQftf-3bJ"
      },
      "source": [
        "Escribe tus conclusiones y respuesta aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqmcljOlPuTZ"
      },
      "source": [
        "### Pregunta 6 (3 ptos.)\n",
        "\n",
        "Ahora entrena un modelo de `boosting` y también evalúalo. Indica nuevamente qué modelo es mejor y analiza el último árbol (si sabes el concepto sabes al por qué hacemos esto), ¿qué conclusiones puedes hacer? (respuestas genéricas sin conexión al caso de uso tendrán automáticamente 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiaRzt7mPuTZ"
      },
      "outputs": [],
      "source": [
        "### Escribe el código aquí ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO7TeVsJPuTZ"
      },
      "source": [
        "Escribe tus conclusiones y respuesta aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRz0Emdg-3bP"
      },
      "source": [
        "### Pregunta 7 (2 ptos.)\n",
        "\n",
        "Ahora entrena un modelo `KNN` y calibra el hiperparámetro del número de vecinos para maximizar el rendimiento de la métrica que has escogido. Compáralo con el rendimiento de los otros modelos e indica cuál es mejor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEo2JGtB-3bP"
      },
      "outputs": [],
      "source": [
        "### Escribe el código aquí ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGlMsXBs-3bV"
      },
      "source": [
        "Escribe tus conclusiones y respuesta aquí"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}